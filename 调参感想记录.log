-------------------------------前期调参猜想（不一定对）--------------------------------------

1. 逐渐改变目标网络 比 隔段时间直接复制网络 更加稳定。

2. 训练时鼓励超越目标，实际测试时到达期望目标即可。若训练时在目标处停止，那么可能出现 取其上者得乎其中 的现象？

3. gym环境通常会有 微小的随机扰动，因此需要 Dropout 层来提高鲁棒性

4. 这种短期的、主要考虑当下状态的环境，应该 短视 一些好，将GAMMA设小一点

5. 为了应对新状况，需要及时训练反馈，因此应该将 MEMORY_SIZE 改小一点（太小则可能导致相关性影响）

----------------------------与pytorch官网代码比对--------------------------------------

6. SmoothL1Loss(Huber) 损失函数比 MSELoss 更加稳定、鲁棒

7. 破案了，之前各vision后期训练效果剧烈恶化是因为梯度爆炸问题，使用 torch.nn.utils.clip_grad_value_ 函数裁剪梯度即可

--------------------------------后期调参感受----------------------------------

8. MEMORY_SIZE 取 BATCH_SIZE 的平方效果比较好





